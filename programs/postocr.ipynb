{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0c128de-4421-4688-9526-75f97cc7168e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Post OCR\n",
    "\n",
    "First task: distil a lexicon of good words from the corpus.\n",
    "\n",
    "Intuitition: make a list of the bi- and trigrams of letters, select the most freqent of these, and weed out the ones that \n",
    "cannot be part of words and are clearly ocr mistakes.\n",
    "\n",
    "Then find all words in the corpus that consist of such bi- and trigrams.\n",
    "\n",
    "We will miss rare words of which no correct form exists in the corpus.\n",
    "\n",
    "We may try to correct such words by replacing their faulty bi- or trigrams by corrected ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1e4bed4f-2211-4800-ba0d-14b3aa1f9532",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce0b38cc-6b52-46c2-8c48-caaca8333f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import collections\n",
    "import re\n",
    "from tf.app import use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3a0e4f3-67e3-4dc0-b5ec-b6141e23f527",
   "metadata": {},
   "outputs": [],
   "source": [
    "POST_DIR = os.path.expanduser(\"~/github/Dans-labs/clariah-dr/postocr\")\n",
    "if not os.path.exists(POST_DIR):\n",
    "    os.makedirs(POST_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "3aaeb077-3005-4a17-afb1-72cf183ed785",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<b title=\"local github\">data:</b> <span title=\"repo clone offline under ~/github\">~/github/Dans-labs/clariah-dr/tf/daghregister/004/0.1</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is Text-Fabric 9.1.6\n",
      "Api reference : https://annotation.github.io/text-fabric/tf/cheatsheet.html\n",
      "\n",
      "13 features found and 0 ignored\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<b>Text-Fabric:</b> <a target=\"_blank\" href=\"https://annotation.github.io/text-fabric/tf/cheatsheet.html\" title=\"text-fabric-api\">Text-Fabric API 9.1.6</a>, no app configured<br><b>Data:</b> Dans-labs/clariah-dr/tf/daghregister/004/0.1<br><b>Features:</b><br><details><summary><b>Dans-labs/clariah-dr/tf/daghregister/004/0.1</b></summary><a target=\"_blank\" href=\"https://github.com/Dans-labs/clariah-dr/tree/master/tf/daghregister/004/0.1\" title=\"~/github/Dans-labs/clariah-dr/tf/daghregister/004/0.1//dayfrom.tf\">dayfrom</a><br><a target=\"_blank\" href=\"https://github.com/Dans-labs/clariah-dr/tree/master/tf/daghregister/004/0.1\" title=\"~/github/Dans-labs/clariah-dr/tf/daghregister/004/0.1//dayto.tf\">dayto</a><br><a target=\"_blank\" href=\"https://github.com/Dans-labs/clariah-dr/tree/master/tf/daghregister/004/0.1\" title=\"~/github/Dans-labs/clariah-dr/tf/daghregister/004/0.1//head.tf\">head</a><br><a target=\"_blank\" href=\"https://github.com/Dans-labs/clariah-dr/tree/master/tf/daghregister/004/0.1\" title=\"~/github/Dans-labs/clariah-dr/tf/daghregister/004/0.1//letters.tf\">letters</a><br><a target=\"_blank\" href=\"https://github.com/Dans-labs/clariah-dr/tree/master/tf/daghregister/004/0.1\" title=\"~/github/Dans-labs/clariah-dr/tf/daghregister/004/0.1//month.tf\">month</a><br><a target=\"_blank\" href=\"https://github.com/Dans-labs/clariah-dr/tree/master/tf/daghregister/004/0.1\" title=\"~/github/Dans-labs/clariah-dr/tf/daghregister/004/0.1//n.tf\">n</a><br><a target=\"_blank\" href=\"https://github.com/Dans-labs/clariah-dr/tree/master/tf/daghregister/004/0.1\" title=\"~/github/Dans-labs/clariah-dr/tf/daghregister/004/0.1//otype.tf\">otype</a><br><a target=\"_blank\" href=\"https://github.com/Dans-labs/clariah-dr/tree/master/tf/daghregister/004/0.1\" title=\"~/github/Dans-labs/clariah-dr/tf/daghregister/004/0.1//punc.tf\">punc</a><br><a target=\"_blank\" href=\"https://github.com/Dans-labs/clariah-dr/tree/master/tf/daghregister/004/0.1\" title=\"~/github/Dans-labs/clariah-dr/tf/daghregister/004/0.1//side.tf\">side</a><br><a target=\"_blank\" href=\"https://github.com/Dans-labs/clariah-dr/tree/master/tf/daghregister/004/0.1\" title=\"~/github/Dans-labs/clariah-dr/tf/daghregister/004/0.1//year.tf\">year</a><br><a target=\"_blank\" href=\"https://github.com/Dans-labs/clariah-dr/tree/master/tf/daghregister/004/0.1\" title=\"~/github/Dans-labs/clariah-dr/tf/daghregister/004/0.1//years.tf\">years</a><br><b><i><a target=\"_blank\" href=\"https://github.com/Dans-labs/clariah-dr/tree/master/tf/daghregister/004/0.1\" title=\"~/github/Dans-labs/clariah-dr/tf/daghregister/004/0.1//oslots.tf\">oslots</a></i></b><br></details>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>tr.tf.ltr, td.tf.ltr, th.tf.ltr { text-align: left ! important;}\n",
       "tr.tf.rtl, td.tf.rtl, th.tf.rtl { text-align: right ! important;}\n",
       "@font-face {\n",
       "  font-family: \"Gentium Plus\";\n",
       "  src: local('Gentium Plus'), local('GentiumPlus'),\n",
       "    url('/server/static/fonts/GentiumPlus-R.woff') format('woff'),\n",
       "    url('https://github.com/annotation/text-fabric/blob/master/tf/server/static/fonts/GentiumPlus-R.woff?raw=true') format('woff');\n",
       "}\n",
       "\n",
       "@font-face {\n",
       "  font-family: \"Ezra SIL\";\n",
       "  src: local('Ezra SIL'), local('EzraSIL'),\n",
       "    url('/server/static/fonts/SILEOT.woff') format('woff'),\n",
       "    url('https://github.com/annotation/text-fabric/blob/master/tf/server/static/fonts/SILEOT.woff?raw=true') format('woff');\n",
       "}\n",
       "\n",
       "@font-face {\n",
       "  font-family: \"SBL Hebrew\";\n",
       "  src: local('SBL Hebrew'), local('SBLHebrew'),\n",
       "    url('/server/static/fonts/SBL_Hbrw.woff') format('woff'),\n",
       "    url('https://github.com/annotation/text-fabric/blob/master/tf/server/static/fonts/SBL_Hbrw.woff?raw=true') format('woff');\n",
       "}\n",
       "\n",
       "@font-face {\n",
       "  font-family: \"Estrangelo Edessa\";\n",
       "  src: local('Estrangelo Edessa'), local('EstrangeloEdessa');\n",
       "    url('/server/static/fonts/SyrCOMEdessa.woff') format('woff'),\n",
       "    url('https://github.com/annotation/text-fabric/blob/master/tf/server/static/fonts/SyrCOMEdessa.woff?raw=true') format('woff');\n",
       "}\n",
       "\n",
       "@font-face {\n",
       "  font-family: AmiriQuran;\n",
       "  font-style: normal;\n",
       "  font-weight: 400;\n",
       "  src: local('Amiri Quran'), local('AmiriQuran'),\n",
       "    url('/server/static/fonts/AmiriQuran.woff') format('woff'),\n",
       "    url('https://github.com/annotation/text-fabric/blob/master/tf/server/static/fonts/AmiriQuran.woff?raw=true') format('woff');\n",
       "}\n",
       "\n",
       "@font-face {\n",
       "  font-family: AmiriQuranColored;\n",
       "  font-style: normal;\n",
       "  font-weight: 400;\n",
       "  src: local('Amiri Quran Colored'), local('AmiriQuranColored'),\n",
       "    url('/server/static/fonts/AmiriQuranColored.woff') format('woff'),\n",
       "    url('https://github.com/annotation/text-fabric/blob/master/tf/server/static/fonts/AmiriQuranColored.woff?raw=true') format('woff');\n",
       "}\n",
       "\n",
       "@font-face {\n",
       "  font-family: \"Santakku\";\n",
       "  src: local('Santakku'),\n",
       "    url('/server/static/fonts/Santakku.woff') format('woff'),\n",
       "    url('https://github.com/annotation/text-fabric/blob/master/tf/server/static/fonts/Santakku.woff?raw=true') format('woff');\n",
       "}\n",
       "\n",
       "@font-face {\n",
       "  font-family: \"SantakkuM\";\n",
       "  src: local('SantakkuM'),\n",
       "    url('/server/static/fonts/SantakkuM.woff') format('woff'),\n",
       "    url('https://github.com/annotation/text-fabric/blob/master/tf/server/static/fonts/SantakkuM.woff?raw=true') format('woff');\n",
       "}\n",
       "/* bypassing some classical notebook settings */\n",
       "div#notebook {\n",
       "  line-height: unset;\n",
       "}\n",
       "/* neutral text */\n",
       ".txtn,.txtn a:visited,.txtn a:link {\n",
       "    font-family: sans-serif;\n",
       "    font-size: medium;\n",
       "    direction: ltr;\n",
       "    unicode-bidi: embed;\n",
       "    text-decoration: none;\n",
       "    color: var(--text-color);\n",
       "}\n",
       "/* transcription text */\n",
       ".txtt,.txtt a:visited,.txtt a:link {\n",
       "    font-family: monospace;\n",
       "    font-size: medium;\n",
       "    direction: ltr;\n",
       "    unicode-bidi: embed;\n",
       "    text-decoration: none;\n",
       "    color: var(--text-color);\n",
       "}\n",
       "/* source text */\n",
       ".txto,.txto a:visited,.txto a:link {\n",
       "    font-family: serif;\n",
       "    font-size: medium;\n",
       "    direction: ltr;\n",
       "    unicode-bidi: embed;\n",
       "    text-decoration: none;\n",
       "    color: var(--text-color);\n",
       "}\n",
       "/* phonetic text */\n",
       ".txtp,.txtp a:visited,.txtp a:link {\n",
       "    font-family: Gentium, sans-serif;\n",
       "    font-size: medium;\n",
       "    direction: ltr;\n",
       "    unicode-bidi: embed;\n",
       "    text-decoration: none;\n",
       "    color: var(--text-color);\n",
       "}\n",
       "/* original script text */\n",
       ".txtu,.txtu a:visited,.txtu a:link {\n",
       "    font-family: Gentium, sans-serif;\n",
       "    font-size: medium;\n",
       "    text-decoration: none;\n",
       "    color: var(--text-color);\n",
       "}\n",
       "/* hebrew */\n",
       ".txtu.hbo,.lex.hbo {\n",
       "    font-family: \"Ezra SIL\", \"SBL Hebrew\", sans-serif;\n",
       "    font-size: large;\n",
       "    direction: rtl ! important;\n",
       "    unicode-bidi: embed;\n",
       "}\n",
       "/* syriac */\n",
       ".txtu.syc,.lex.syc {\n",
       "    font-family: \"Estrangelo Edessa\", sans-serif;\n",
       "    font-size: medium;\n",
       "    direction: rtl ! important;\n",
       "    unicode-bidi: embed;\n",
       "}\n",
       "/* neo aramaic */\n",
       ".txtu.cld,.lex.cld {\n",
       "    font-family: \"CharisSIL-R\", sans-serif;\n",
       "    font-size: medium;\n",
       "    direction: ltr ! important;\n",
       "    unicode-bidi: embed;\n",
       "}\n",
       "/* standard arabic */\n",
       ".txtu.ara,.lex.ara {\n",
       "    font-family: \"AmiriQuran\", sans-serif;\n",
       "    font-size: large;\n",
       "    direction: rtl ! important;\n",
       "    unicode-bidi: embed;\n",
       "}\n",
       "/* cuneiform */\n",
       ".txtu.akk,.lex.akk {\n",
       "    font-family: Santakku, sans-serif;\n",
       "    font-size: large;\n",
       "    direction: ltr ! important;\n",
       "    unicode-bidi: embed;\n",
       "}\n",
       "/* greek */\n",
       ".txtu.grc,.lex.grc a:link {\n",
       "    font-family: Gentium, sans-serif;\n",
       "    font-size: medium;\n",
       "    direction: ltr ! important;\n",
       "    unicode-bidi: embed;\n",
       "}\n",
       "a:hover {\n",
       "    text-decoration: underline | important;\n",
       "    color: #0000ff | important;\n",
       "}\n",
       ".ltr {\n",
       "    direction: ltr ! important;\n",
       "    unicode-bidi: embed;\n",
       "}\n",
       ".rtl {\n",
       "    direction: rtl ! important;\n",
       "    unicode-bidi: embed;\n",
       "}\n",
       ".ubd {\n",
       "    unicode-bidi: embed;\n",
       "}\n",
       ".col {\n",
       "   display: inline-block;\n",
       "}\n",
       ".features {\n",
       "    font-family: monospace;\n",
       "    font-size: medium;\n",
       "    font-weight: bold;\n",
       "    color: var(--features);\n",
       "    display: flex;\n",
       "    flex-flow: column nowrap;\n",
       "    justify-content: flex-start;\n",
       "    align-items: flex-start;\n",
       "    align-content: flex-start;\n",
       "    padding: 2px;\n",
       "    margin: 2px;\n",
       "    direction: ltr;\n",
       "    unicode-bidi: embed;\n",
       "    border: var(--meta-width) solid var(--meta-color);\n",
       "    border-radius: var(--meta-width);\n",
       "}\n",
       ".features div,.features span {\n",
       "    padding: 0;\n",
       "    margin: -2px 0;\n",
       "}\n",
       ".features .f {\n",
       "    font-family: sans-serif;\n",
       "    font-size: small;\n",
       "    font-weight: normal;\n",
       "    color: #5555bb;\n",
       "}\n",
       ".features .xft {\n",
       "  color: #000000;\n",
       "  background-color: #eeeeee;\n",
       "  font-size: medium;\n",
       "  margin: 2px 0px;\n",
       "}\n",
       ".features .xft .f {\n",
       "  color: #000000;\n",
       "  background-color: #eeeeee;\n",
       "  font-size: small;\n",
       "  font-weight: normal;\n",
       "}\n",
       ".tfsechead {\n",
       "    font-family: sans-serif;\n",
       "    font-size: small;\n",
       "    font-weight: bold;\n",
       "    color: var(--tfsechead);\n",
       "    unicode-bidi: embed;\n",
       "    text-align: start;\n",
       "}\n",
       ".structure {\n",
       "    font-family: sans-serif;\n",
       "    font-size: small;\n",
       "    font-weight: bold;\n",
       "    color: var(--structure);\n",
       "    unicode-bidi: embed;\n",
       "    text-align: start;\n",
       "}\n",
       ".comments {\n",
       "    display: flex;\n",
       "    justify-content: flex-start;\n",
       "    align-items: flex-start;\n",
       "    align-content: flex-start;\n",
       "    flex-flow: column nowrap;\n",
       "}\n",
       ".nd, a:link.nd {\n",
       "    font-family: sans-serif;\n",
       "    font-size: small;\n",
       "    color: var(--node);\n",
       "    vertical-align: super;\n",
       "    direction: ltr ! important;\n",
       "    unicode-bidi: embed;\n",
       "}\n",
       ".lex {\n",
       "  color: var(--lex-color);;\n",
       "}\n",
       ".children,.children.ltr {\n",
       "    display: flex;\n",
       "    border: 0;\n",
       "    background-color: #ffffff;\n",
       "    justify-content: flex-start;\n",
       "    align-items: flex-start;\n",
       "    align-content: flex-start;\n",
       "}\n",
       ".children.stretch {\n",
       "    align-items: stretch;\n",
       "}\n",
       ".children.hor {\n",
       "    flex-flow: row nowrap;\n",
       "}\n",
       ".children.hor.wrap {\n",
       "    flex-flow: row wrap;\n",
       "}\n",
       ".children.ver {\n",
       "    flex-flow: column nowrap;\n",
       "}\n",
       ".children.ver.wrap {\n",
       "    flex-flow: column wrap;\n",
       "}\n",
       ".contnr {\n",
       "    width: fit-content;\n",
       "    display: flex;\n",
       "    justify-content: flex-start;\n",
       "    align-items: flex-start;\n",
       "    align-content: flex-start;\n",
       "    flex-flow: column nowrap;\n",
       "    background: #ffffff none repeat scroll 0 0;\n",
       "    padding:  10px 2px 2px 2px;\n",
       "    margin: 16px 2px 2px 2px;\n",
       "    border-style: solid;\n",
       "    font-size: small;\n",
       "}\n",
       ".contnr.trm {\n",
       "    background-attachment: local;\n",
       "}\n",
       ".contnr.cnul {\n",
       "    padding:  0;\n",
       "    margin: 0;\n",
       "    border-style: solid;\n",
       "    font-size: xx-small;\n",
       "}\n",
       ".contnr.cnul,.lbl.cnul {\n",
       "    border-color: var(--border-color-nul);\n",
       "    border-width: var(--border-width-nul);\n",
       "    border-radius: var(--border-width-nul);\n",
       "}\n",
       ".contnr.c0,.lbl.c0 {\n",
       "    border-color: var(--border-color0);\n",
       "    border-width: var(--border-width0);\n",
       "    border-radius: var(--border-width0);\n",
       "}\n",
       ".contnr.c1,.lbl.c1 {\n",
       "    border-color: var(--border-color1);\n",
       "    border-width: var(--border-width1);\n",
       "    border-radius: var(--border-width1);\n",
       "}\n",
       ".contnr.c2,.lbl.c2 {\n",
       "    border-color: var(--border-color2);\n",
       "    border-width: var(--border-width2);\n",
       "    border-radius: var(--border-width2);\n",
       "}\n",
       ".contnr.c3,.lbl.c3 {\n",
       "    border-color: var(--border-color3);\n",
       "    border-width: var(--border-width3);\n",
       "    border-radius: var(--border-width3);\n",
       "}\n",
       ".contnr.c4,.lbl.c4 {\n",
       "    border-color: var(--border-color4);\n",
       "    border-width: var(--border-width4);\n",
       "    border-radius: var(--border-width4);\n",
       "}\n",
       "span.plain {\n",
       "    display: inline-block;\n",
       "    white-space: pre-wrap;\n",
       "}\n",
       ".plain {\n",
       "    background-color: #ffffff;\n",
       "}\n",
       ".plain.l,.contnr.l,.contnr.l>.lbl {\n",
       "    border-left-style: dotted\n",
       "}\n",
       ".plain.r,.contnr.r,.contnr.r>.lbl {\n",
       "    border-right-style: dotted\n",
       "}\n",
       ".plain.lno,.contnr.lno,.contnr.lno>.lbl {\n",
       "    border-left-style: none\n",
       "}\n",
       ".plain.rno,.contnr.rno,.contnr.rno>.lbl {\n",
       "    border-right-style: none\n",
       "}\n",
       ".plain.l {\n",
       "    padding-left: 4px;\n",
       "    margin-left: 2px;\n",
       "    border-width: var(--border-width-plain);\n",
       "}\n",
       ".plain.r {\n",
       "    padding-right: 4px;\n",
       "    margin-right: 2px;\n",
       "    border-width: var(--border-width-plain);\n",
       "}\n",
       ".lbl {\n",
       "    font-family: monospace;\n",
       "    margin-top: -24px;\n",
       "    margin-left: 20px;\n",
       "    background: #ffffff none repeat scroll 0 0;\n",
       "    padding: 0 6px;\n",
       "    border-style: solid;\n",
       "    display: block;\n",
       "    color: var(--label)\n",
       "}\n",
       ".lbl.trm {\n",
       "    background-attachment: local;\n",
       "    margin-top: 2px;\n",
       "    margin-left: 2px;\n",
       "    padding: 2px 2px;\n",
       "    border-style: none;\n",
       "}\n",
       ".lbl.cnul {\n",
       "    font-size: xx-small;\n",
       "}\n",
       ".lbl.c0 {\n",
       "    font-size: small;\n",
       "}\n",
       ".lbl.c1 {\n",
       "    font-size: small;\n",
       "}\n",
       ".lbl.c2 {\n",
       "    font-size: medium;\n",
       "}\n",
       ".lbl.c3 {\n",
       "    font-size: medium;\n",
       "}\n",
       ".lbl.c4 {\n",
       "    font-size: large;\n",
       "}\n",
       ".occs, a:link.occs {\n",
       "    font-size: small;\n",
       "}\n",
       "\n",
       "/* PROVENANCE */\n",
       "\n",
       "div.prov {\n",
       "\tmargin: 40px;\n",
       "\tpadding: 20px;\n",
       "\tborder: 2px solid var(--fog-rim);\n",
       "}\n",
       "div.pline {\n",
       "\tdisplay: flex;\n",
       "\tflex-flow: row nowrap;\n",
       "\tjustify-content: stretch;\n",
       "\talign-items: baseline;\n",
       "}\n",
       "div.p2line {\n",
       "\tmargin-left: 2em;\n",
       "\tdisplay: flex;\n",
       "\tflex-flow: row nowrap;\n",
       "\tjustify-content: stretch;\n",
       "\talign-items: baseline;\n",
       "}\n",
       "div.psline {\n",
       "\tdisplay: flex;\n",
       "\tflex-flow: row nowrap;\n",
       "\tjustify-content: stretch;\n",
       "\talign-items: baseline;\n",
       "\tbackground-color: var(--gold-mist-back);\n",
       "}\n",
       "div.pname {\n",
       "\tflex: 0 0 5rem;\n",
       "\tfont-weight: bold;\n",
       "}\n",
       "div.pval {\n",
       "    flex: 1 1 auto;\n",
       "}\n",
       "\n",
       ":root {\n",
       "\t--node:               hsla(120, 100%,  20%, 1.0  );\n",
       "\t--label:              hsla(  0, 100%,  20%, 1.0  );\n",
       "\t--tfsechead:          hsla(  0, 100%,  25%, 1.0  );\n",
       "\t--structure:          hsla(120, 100%,  25%, 1.0  );\n",
       "\t--features:           hsla(  0,   0%,  30%, 1.0  );\n",
       "  --text-color:         hsla( 60,  80%,  10%, 1.0  );\n",
       "  --lex-color:          hsla(220,  90%,  60%, 1.0  );\n",
       "  --meta-color:         hsla(  0,   0%,  90%, 0.7  );\n",
       "  --meta-width:         3px;\n",
       "  --border-color-nul:   hsla(  0,   0%,  90%, 0.5  );\n",
       "  --border-color0:      hsla(  0,   0%,  90%, 0.9  );\n",
       "  --border-color1:      hsla(  0,   0%,  80%, 0.9  );\n",
       "  --border-color2:      hsla(  0,   0%,  70%, 0.9  );\n",
       "  --border-color3:      hsla(  0,   0%,  80%, 0.8  );\n",
       "  --border-color4:      hsla(  0,   0%,  60%, 0.9  );\n",
       "  --border-width-nul:   2px;\n",
       "  --border-width0:      2px;\n",
       "  --border-width1:      3px;\n",
       "  --border-width2:      4px;\n",
       "  --border-width3:      6px;\n",
       "  --border-width4:      5px;\n",
       "  --border-width-plain: 2px;\n",
       "}\n",
       ".hl {\n",
       "  background-color: var(--hl-strong);\n",
       "}\n",
       "span.hl {\n",
       "\tbackground-color: var(--hl-strong);\n",
       "\tborder-width: 0;\n",
       "\tborder-radius: 2px;\n",
       "\tborder-style: solid;\n",
       "}\n",
       "div.contnr.hl,div.lbl.hl {\n",
       "  background-color: var(--hl-strong);\n",
       "}\n",
       "div.contnr.hl {\n",
       "  border-color: var(--hl-rim) ! important;\n",
       "\tborder-width: 4px ! important;\n",
       "}\n",
       "\n",
       "span.hlbx {\n",
       "\tborder-color: var(--hl-rim);\n",
       "\tborder-width: 4px ! important;\n",
       "\tborder-style: solid;\n",
       "\tborder-radius: 6px;\n",
       "  padding: 4px;\n",
       "  margin: 4px;\n",
       "}\n",
       "\n",
       "span.plain {\n",
       "  display: inline-block;\n",
       "  white-space: pre-wrap;\n",
       "}\n",
       "\n",
       ":root {\n",
       "\t--hl-strong:        hsla( 60, 100%,  70%, 0.9  );\n",
       "\t--hl-rim:           hsla( 55,  80%,  50%, 1.0  );\n",
       "}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div><b>Text-Fabric API:</b> names <a target=\"_blank\" href=\"https://annotation.github.io/text-fabric/tf/cheatsheet.html\" title=\"doc\">N F E L T S C TF</a> directly usable</div><hr>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "A = use(\"Dans-labs/clariah-dr/tf/daghregister/004/0.1:clone\", checkout=\"clone\", hoist=globals())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4ab3d49-e9c8-42a8-a8c4-186110dfa29d",
   "metadata": {},
   "source": [
    "Generic function to show a frequency distribution of data.\n",
    "\n",
    "Data is a dict where the keys are frequencies and the values are the amounts of items that have that frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "fa0b7884-d919-4df1-b282-bd6688fe8d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def showDistribution(data, itemLabel, amountLabel):\n",
    "    buckets = collections.Counter()\n",
    "    for (freq, nItems) in data.items():\n",
    "        bucket = freq\n",
    "        for n in range(7, 0, -1):\n",
    "            limit = 10 ** n\n",
    "            if freq >= limit:\n",
    "                bucket = int(freq / limit) * limit\n",
    "                break\n",
    "        buckets[bucket] += nItems\n",
    "    for (bucket, nItems) in sorted(buckets.items(), key=lambda x: -x[0]):\n",
    "        plural = \" \" if nItems == 1 else \"s\"\n",
    "        print(f\"{nItems:>7} {itemLabel}{plural} with {amountLabel} >= {bucket:>7}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e52d67-b2c0-4ab1-b270-a8b32a7ec9d6",
   "metadata": {},
   "source": [
    "We walk through the corpus and harvest the 2- and 3- lettergrams.\n",
    "For each gram we store information about the forms they occur in and how often they occur overall.\n",
    "\n",
    "More precisely:\n",
    "\n",
    "* `GRAM[n][\"form\"]` gives per n-gram a dict keyed by word forms that contain it and how often"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "0d7c84fb-d637-47f0-8c4a-0e14f105ddc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "WORD_OCCS = collections.defaultdict(list)\n",
    "GRAM = {\n",
    "    2: collections.defaultdict(list),\n",
    "    3: collections.defaultdict(list),\n",
    "}\n",
    "\n",
    "GRAM_INDEX = {}\n",
    "\n",
    "CHARS = collections.Counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "f96485fe-52ec-4634-a61e-d2957e7b5de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getGrams():\n",
    "    CHARS.clear()\n",
    "    WORD_OCCS.clear()\n",
    "    GRAM_INDEX.clear()\n",
    "    for (n, grams) in GRAM.items():\n",
    "        grams.clear()\n",
    "            \n",
    "    ns = list(GRAM)\n",
    "    \n",
    "    allWords = F.otype.s(\"word\")\n",
    "    with open(f\"{POST_DIR}/forms.tsv\", \"w\") as fh:\n",
    "        for w in allWords:\n",
    "            letters = F.letters.v(w)\n",
    "            for c in letters:\n",
    "                CHARS[c] += 1\n",
    "            WORD_OCCS[letters].append(w)\n",
    "            (volume, page, line) = T.sectionFromNode(w)\n",
    "            fh.write(f\"{page}\\t{line}\\t{letters}\\n\")\n",
    "            lower = f\" {letters.lower()} \"\n",
    "            if letters in GRAM_INDEX:\n",
    "                continue\n",
    "            index = collections.defaultdict(list)\n",
    "            GRAM_INDEX[letters] = index\n",
    "            myGrams = {n: set() for n in GRAM}\n",
    "            for (i, c) in enumerate(letters):\n",
    "                for n in ns:\n",
    "                    first = i - n + 1\n",
    "                    if first >= 0:\n",
    "                        gram = letters[first:i + 1]\n",
    "                        myGrams[n].add(gram)\n",
    "\n",
    "            for (n, grams) in myGrams.items():\n",
    "                for gram in grams:\n",
    "                    GRAM[n][gram].append(letters)\n",
    "                    index[n].append(gram)\n",
    "            \n",
    "    print(\"CHARACTERS:\")\n",
    "    for (c, freq) in sorted(CHARS.items(), key=lambda x: (-x[1], x[0])):\n",
    "        print(f\"{c} {freq:>7}\")\n",
    "    charRep = \"\".join(sorted(CHARS))\n",
    "    print(charRep)\n",
    "    print(f\"{len(allWords)} words in {len(WORD_OCCS)} distinct forms\")\n",
    "    for (n, grams) in GRAM.items():\n",
    "        print(f\"{len(grams)} {n}-grams\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4b7ecc82-4802-4b9f-9671-af34bd07726f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CHARACTERS:\n",
      "e  245074\n",
      "n  129421\n",
      "a   85020\n",
      "t   74078\n",
      "o   72160\n",
      "d   70758\n",
      "r   69171\n",
      "s   56507\n",
      "l   40368\n",
      "c   37938\n",
      "i   37431\n",
      "g   33067\n",
      "v   29973\n",
      "h   29719\n",
      "m   27402\n",
      "y   22528\n",
      "u   19452\n",
      "p   16292\n",
      "w   14298\n",
      "b   14233\n",
      "k   12423\n",
      "f    7391\n",
      "'    3866\n",
      "0    3297\n",
      "j    3203\n",
      "C    2974\n",
      "1    2866\n",
      "2    2694\n",
      "M    2161\n",
      "G    2066\n",
      "E    1994\n",
      "S    1910\n",
      "P    1472\n",
      "4    1460\n",
      "D    1451\n",
      "B    1379\n",
      "x    1379\n",
      "5    1368\n",
      "J    1367\n",
      "A    1352\n",
      "3    1289\n",
      "q    1199\n",
      "H    1190\n",
      "T    1111\n",
      "6    1101\n",
      "8    1029\n",
      "7     873\n",
      "N     746\n",
      "O     725\n",
      "L     718\n",
      "z     657\n",
      "9     605\n",
      "U     598\n",
      "W     596\n",
      "V     579\n",
      "I     566\n",
      "(     541\n",
      "R     509\n",
      "^     439\n",
      "K     310\n",
      "*     273\n",
      "F     265\n",
      ".     231\n",
      "Q     168\n",
      "ü     158\n",
      "é     154\n",
      "ë      98\n",
      ")      94\n",
      ":      88\n",
      "Z      70\n",
      "-      68\n",
      "Y      57\n",
      "/      53\n",
      "ê      52\n",
      "è      46\n",
      "ï      40\n",
      "Ë      35\n",
      "ó      28\n",
      "ö      26\n",
      "Ü      22\n",
      "[      19\n",
      "\\      18\n",
      "«      18\n",
      "X      16\n",
      "\"      15\n",
      ";      15\n",
      "<      15\n",
      ">      15\n",
      "!      14\n",
      "|       8\n",
      "£       6\n",
      "$       4\n",
      "&       4\n",
      "?       4\n",
      "]       4\n",
      "Ö       4\n",
      "»       3\n",
      "È       3\n",
      "Ó       3\n",
      "•       3\n",
      "#       2\n",
      "}       2\n",
      "~       2\n",
      "%       1\n",
      "_       1\n",
      "§       1\n",
      "®       1\n",
      "°       1\n",
      "É       1\n",
      "Ï       1\n",
      "€       1\n",
      "™       1\n",
      "!\"#$%&'()*-./0123456789:;<>?ABCDEFGHIJKLMNOPQRSTUVWXYZ[\\]^_abcdefghijklmnopqrstuvwxyz|}~£§«®°»ÈÉËÏÓÖÜèéêëïóöü•€™\n",
      "229297 words in 24717 distinct forms\n",
      "2421 2-grams\n",
      "11006 3-grams\n"
     ]
    }
   ],
   "source": [
    "getGrams()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8cfa35-6aa8-4c58-b835-dd550ccc3245",
   "metadata": {},
   "source": [
    "We have the bi- and trigrams now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb7a641-bf9b-46ca-a9ca-a39942863fe0",
   "metadata": {},
   "source": [
    "# OCR key\n",
    "\n",
    "I compute the OCR keys of all forms, in order to see whether illegal words have counterparts with the same OCR key that are legal.\n",
    "If so, we con choose the one with the minimum edit distance as a correction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "6201df7c-9ebf-4a92-9975-3cc0ebe0b50a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'o1i3v2'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CHAR_CLASSES = \"\"\"\n",
    "*0 •™_~\"[\n",
    "i1 fijklrtBDEFIJKLPRT1!ïÈÉËÏ£|!\\\n",
    "i2 nhuHNUüÜ«°]\n",
    "i3 mM\n",
    "o1 abdgopqOQ690óöÓÖ()»}#&><^\n",
    "c1 ecCGèéêë€*®?\n",
    "v1 vxyVXY\n",
    "v2 ww\n",
    "s1 sS5$§\n",
    "z1 zZ\n",
    "21 2%\n",
    "a1 A\n",
    "\"\"\"\n",
    "\n",
    "OCR_KEY = {}\n",
    "\n",
    "for line in CHAR_CLASSES.strip().split(\"\\n\"):\n",
    "    (clsCard, chars) = line.split(\" \", 1)\n",
    "    (cls, card) = clsCard\n",
    "    for c in chars:\n",
    "        OCR_KEY[c] = (cls, card)\n",
    "        \n",
    "        \n",
    "def getOcrKey(letters):\n",
    "    clses = []\n",
    "    for c in letters:\n",
    "        (cls, card) = OCR_KEY.get(c, (c, 1))\n",
    "        if clses and clses[-1][0] == cls:\n",
    "            clses[-1][1] += card\n",
    "        else:\n",
    "            clses.append([cls, card])\n",
    "    return \"\".join(f\"{cls}{card}\" for (cls, card) in clses)\n",
    "\n",
    "getOcrKey(\"amw\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9d8970c-eca7-4287-9e55-cbce85b3d4ac",
   "metadata": {},
   "source": [
    "Let's make an index of the word forms by ocr key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "10f80383-176e-4ca4-a9e2-44a240435683",
   "metadata": {},
   "outputs": [],
   "source": [
    "WORD_OCR = collections.defaultdict(list)\n",
    "\n",
    "def makeOcrIndex():\n",
    "    WORD_OCR.clear()\n",
    "    \n",
    "    for word in WORD_OCCS:\n",
    "        WORD_OCR[getOcrKey(word)].append(word)\n",
    "        \n",
    "    print(f\"{len(WORD_OCCS)} words clustered into {len(WORD_OCR)} ocr keys\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4cfe5ff6-2063-4e16-9e81-e735621d7662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24717 words clustered into 16827 ocr keys\n"
     ]
    }
   ],
   "source": [
    "makeOcrIndex()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc137c5-8a10-4f8c-bea5-a786770bafd4",
   "metadata": {},
   "source": [
    "Let's the amount of word froms they occur in.\n",
    "We show it in two ways:\n",
    "\n",
    "* only looking at the amount of distinct words the n-grams occur in\n",
    "* taking into account the frequencies of the words the n-grams occur in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d142fb8d-2c68-4a50-b103-238e16e1a4a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def occFreq(n, gram):\n",
    "    forms = GRAM[n][gram]\n",
    "    return sum(len(WORD_OCCS[form]) for form in forms)\n",
    "               \n",
    "               \n",
    "def distFreq():\n",
    "    for (n, grams) in GRAM.items():\n",
    "        fileName = f\"{n}-gram-info.tsv\"\n",
    "        itemLabel = f\"{n}-gram\"\n",
    "        amountLabel = f\"frequency of word occurrences\"\n",
    "        print(f\"{len(grams)} letter {itemLabel}s by {amountLabel}\")\n",
    "        distribution = collections.Counter()\n",
    "        with open(f\"{POST_DIR}/{fileName}\", \"w\") as fh:\n",
    "            for (gr, forms) in sorted(grams.items(), key=lambda x: -occFreq(n, x[0])):\n",
    "                examples = list(forms)[0:3]\n",
    "                exampleRep = \"\\t\".join(examples)\n",
    "                nForms = len(forms)\n",
    "                nOccs = occFreq(n, gr)\n",
    "                distribution[nOccs] += 1\n",
    "                fh.write(f\"{gr}\\t{nForms}\\t{nOccs}\\t{exampleRep}\\n\")\n",
    "        showDistribution(distribution, itemLabel, amountLabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "53875f69-888a-4e5b-9c17-bdb3150afac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1746 letter 2-grams by frequency of word occurrences\n",
      "      2 2-grams with frequency of word occurrences >=   60000\n",
      "      1 2-gram  with frequency of word occurrences >=   50000\n",
      "      1 2-gram  with frequency of word occurrences >=   40000\n",
      "      2 2-grams with frequency of word occurrences >=   30000\n",
      "      4 2-grams with frequency of word occurrences >=   20000\n",
      "     24 2-grams with frequency of word occurrences >=   10000\n",
      "      5 2-grams with frequency of word occurrences >=    9000\n",
      "      6 2-grams with frequency of word occurrences >=    8000\n",
      "      5 2-grams with frequency of word occurrences >=    7000\n",
      "      6 2-grams with frequency of word occurrences >=    6000\n",
      "     18 2-grams with frequency of word occurrences >=    5000\n",
      "     15 2-grams with frequency of word occurrences >=    4000\n",
      "     27 2-grams with frequency of word occurrences >=    3000\n",
      "     28 2-grams with frequency of word occurrences >=    2000\n",
      "     69 2-grams with frequency of word occurrences >=    1000\n",
      "     12 2-grams with frequency of word occurrences >=     900\n",
      "      8 2-grams with frequency of word occurrences >=     800\n",
      "     16 2-grams with frequency of word occurrences >=     700\n",
      "     15 2-grams with frequency of word occurrences >=     600\n",
      "     20 2-grams with frequency of word occurrences >=     500\n",
      "     18 2-grams with frequency of word occurrences >=     400\n",
      "     27 2-grams with frequency of word occurrences >=     300\n",
      "     35 2-grams with frequency of word occurrences >=     200\n",
      "     57 2-grams with frequency of word occurrences >=     100\n",
      "     17 2-grams with frequency of word occurrences >=      90\n",
      "      6 2-grams with frequency of word occurrences >=      80\n",
      "     14 2-grams with frequency of word occurrences >=      70\n",
      "     14 2-grams with frequency of word occurrences >=      60\n",
      "     21 2-grams with frequency of word occurrences >=      50\n",
      "     31 2-grams with frequency of word occurrences >=      40\n",
      "     53 2-grams with frequency of word occurrences >=      30\n",
      "     68 2-grams with frequency of word occurrences >=      20\n",
      "    126 2-grams with frequency of word occurrences >=      10\n",
      "     22 2-grams with frequency of word occurrences >=       9\n",
      "     40 2-grams with frequency of word occurrences >=       8\n",
      "     20 2-grams with frequency of word occurrences >=       7\n",
      "     31 2-grams with frequency of word occurrences >=       6\n",
      "     43 2-grams with frequency of word occurrences >=       5\n",
      "     65 2-grams with frequency of word occurrences >=       4\n",
      "     95 2-grams with frequency of word occurrences >=       3\n",
      "    171 2-grams with frequency of word occurrences >=       2\n",
      "    488 2-grams with frequency of word occurrences >=       1\n",
      "10703 letter 3-grams by frequency of word occurrences\n",
      "      1 3-gram  with frequency of word occurrences >=   40000\n",
      "      1 3-gram  with frequency of word occurrences >=   20000\n",
      "      6 3-grams with frequency of word occurrences >=   10000\n",
      "      4 3-grams with frequency of word occurrences >=    8000\n",
      "      3 3-grams with frequency of word occurrences >=    7000\n",
      "      3 3-grams with frequency of word occurrences >=    6000\n",
      "     11 3-grams with frequency of word occurrences >=    5000\n",
      "     17 3-grams with frequency of word occurrences >=    4000\n",
      "     16 3-grams with frequency of word occurrences >=    3000\n",
      "     56 3-grams with frequency of word occurrences >=    2000\n",
      "    134 3-grams with frequency of word occurrences >=    1000\n",
      "     25 3-grams with frequency of word occurrences >=     900\n",
      "     37 3-grams with frequency of word occurrences >=     800\n",
      "     40 3-grams with frequency of word occurrences >=     700\n",
      "     56 3-grams with frequency of word occurrences >=     600\n",
      "     68 3-grams with frequency of word occurrences >=     500\n",
      "     97 3-grams with frequency of word occurrences >=     400\n",
      "    144 3-grams with frequency of word occurrences >=     300\n",
      "    210 3-grams with frequency of word occurrences >=     200\n",
      "    458 3-grams with frequency of word occurrences >=     100\n",
      "     77 3-grams with frequency of word occurrences >=      90\n",
      "     82 3-grams with frequency of word occurrences >=      80\n",
      "    108 3-grams with frequency of word occurrences >=      70\n",
      "    117 3-grams with frequency of word occurrences >=      60\n",
      "    127 3-grams with frequency of word occurrences >=      50\n",
      "    185 3-grams with frequency of word occurrences >=      40\n",
      "    233 3-grams with frequency of word occurrences >=      30\n",
      "    398 3-grams with frequency of word occurrences >=      20\n",
      "    787 3-grams with frequency of word occurrences >=      10\n",
      "    115 3-grams with frequency of word occurrences >=       9\n",
      "    151 3-grams with frequency of word occurrences >=       8\n",
      "    192 3-grams with frequency of word occurrences >=       7\n",
      "    242 3-grams with frequency of word occurrences >=       6\n",
      "    300 3-grams with frequency of word occurrences >=       5\n",
      "    364 3-grams with frequency of word occurrences >=       4\n",
      "    682 3-grams with frequency of word occurrences >=       3\n",
      "   1251 3-grams with frequency of word occurrences >=       2\n",
      "   3905 3-grams with frequency of word occurrences >=       1\n"
     ]
    }
   ],
   "source": [
    "distFreq()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "877b14e2-9b46-4e8a-b258-c128d9be66d3",
   "metadata": {},
   "source": [
    "# Legal grams\n",
    "\n",
    "We try to weed out grams that cannot occur in real words.\n",
    "\n",
    "We leave out grams that have a too low frequency and grams that have illegal characters in them.\n",
    "\n",
    "We might leave out legal grams in this process!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "61b8d34d-9a43-4b30-a152-aaf0b21cbb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "LIMIT = {\n",
    "    2: 10,\n",
    "    3: 10,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6b130f62-bd40-4e2e-b958-ac4e3ba1c8fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEGAL_GRAM = {\n",
    "    2: set(),\n",
    "    3: set(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "96f41f2f-90c4-4248-a049-52ed29183118",
   "metadata": {},
   "outputs": [],
   "source": [
    "impureRe = re.compile(r\"\"\"[\\\\/(^0-9.,*:<>()•«!\\[\\]\"]\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "56673eeb-6aa0-49c3-87e6-43b6934a0388",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLegals():\n",
    "    for n in GRAM:\n",
    "        LEGAL_GRAM[n] = set()\n",
    "        \n",
    "    for (n, grams) in GRAM.items():\n",
    "        limit = LIMIT[n]\n",
    "        legals = LEGAL_GRAM[n]\n",
    "        for (gram, forms) in grams.items():\n",
    "            freq = occFreq(n, gram)\n",
    "            if freq >= limit and not impureRe.search(gram):\n",
    "                legals.add(gram)\n",
    "        \n",
    "    for (n, grams) in LEGAL_GRAM.items():\n",
    "        print(f\"{len(grams)} legal {n}-grams\")\n",
    "        with open(f\"{POST_DIR}/legal-{n}-grams.tsv\", \"w\") as fh:\n",
    "            distribution = collections.Counter()\n",
    "            for gram in sorted(grams, key=lambda x: occFreq(n, x)):\n",
    "                freq = occFreq(n, gram)\n",
    "                distribution[freq] += 1\n",
    "                fh.write(f\"{gram}\\t{freq}\\n\")\n",
    "            showDistribution(distribution, f\"legal {n}-gram\", \"occurrences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cd5caec5-1dba-41a5-9c93-0dc5eca35322",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "570 legal 2-grams\n",
      "      2 legal 2-grams with occurrences >=   60000\n",
      "      1 legal 2-gram  with occurrences >=   50000\n",
      "      1 legal 2-gram  with occurrences >=   40000\n",
      "      2 legal 2-grams with occurrences >=   30000\n",
      "      4 legal 2-grams with occurrences >=   20000\n",
      "     24 legal 2-grams with occurrences >=   10000\n",
      "      5 legal 2-grams with occurrences >=    9000\n",
      "      6 legal 2-grams with occurrences >=    8000\n",
      "      5 legal 2-grams with occurrences >=    7000\n",
      "      6 legal 2-grams with occurrences >=    6000\n",
      "     18 legal 2-grams with occurrences >=    5000\n",
      "     15 legal 2-grams with occurrences >=    4000\n",
      "     27 legal 2-grams with occurrences >=    3000\n",
      "     26 legal 2-grams with occurrences >=    2000\n",
      "     67 legal 2-grams with occurrences >=    1000\n",
      "     12 legal 2-grams with occurrences >=     900\n",
      "      7 legal 2-grams with occurrences >=     800\n",
      "     13 legal 2-grams with occurrences >=     700\n",
      "     12 legal 2-grams with occurrences >=     600\n",
      "     14 legal 2-grams with occurrences >=     500\n",
      "     15 legal 2-grams with occurrences >=     400\n",
      "     24 legal 2-grams with occurrences >=     300\n",
      "     30 legal 2-grams with occurrences >=     200\n",
      "     43 legal 2-grams with occurrences >=     100\n",
      "     12 legal 2-grams with occurrences >=      90\n",
      "      4 legal 2-grams with occurrences >=      80\n",
      "      9 legal 2-grams with occurrences >=      70\n",
      "      8 legal 2-grams with occurrences >=      60\n",
      "     13 legal 2-grams with occurrences >=      50\n",
      "     19 legal 2-grams with occurrences >=      40\n",
      "     26 legal 2-grams with occurrences >=      30\n",
      "     29 legal 2-grams with occurrences >=      20\n",
      "     71 legal 2-grams with occurrences >=      10\n",
      "3239 legal 3-grams\n",
      "      1 legal 3-gram  with occurrences >=   40000\n",
      "      1 legal 3-gram  with occurrences >=   20000\n",
      "      6 legal 3-grams with occurrences >=   10000\n",
      "      4 legal 3-grams with occurrences >=    8000\n",
      "      3 legal 3-grams with occurrences >=    7000\n",
      "      3 legal 3-grams with occurrences >=    6000\n",
      "     11 legal 3-grams with occurrences >=    5000\n",
      "     17 legal 3-grams with occurrences >=    4000\n",
      "     16 legal 3-grams with occurrences >=    3000\n",
      "     56 legal 3-grams with occurrences >=    2000\n",
      "    134 legal 3-grams with occurrences >=    1000\n",
      "     25 legal 3-grams with occurrences >=     900\n",
      "     37 legal 3-grams with occurrences >=     800\n",
      "     39 legal 3-grams with occurrences >=     700\n",
      "     55 legal 3-grams with occurrences >=     600\n",
      "     67 legal 3-grams with occurrences >=     500\n",
      "     97 legal 3-grams with occurrences >=     400\n",
      "    140 legal 3-grams with occurrences >=     300\n",
      "    200 legal 3-grams with occurrences >=     200\n",
      "    435 legal 3-grams with occurrences >=     100\n",
      "     70 legal 3-grams with occurrences >=      90\n",
      "     75 legal 3-grams with occurrences >=      80\n",
      "    101 legal 3-grams with occurrences >=      70\n",
      "    111 legal 3-grams with occurrences >=      60\n",
      "    118 legal 3-grams with occurrences >=      50\n",
      "    177 legal 3-grams with occurrences >=      40\n",
      "    215 legal 3-grams with occurrences >=      30\n",
      "    356 legal 3-grams with occurrences >=      20\n",
      "    669 legal 3-grams with occurrences >=      10\n"
     ]
    }
   ],
   "source": [
    "getLegals()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b46c7cd-ad8e-41dd-9564-b43847570693",
   "metadata": {},
   "source": [
    "# Legal words\n",
    "\n",
    "We now distil the words that are legal, by selecting the words whose bi- and trigrams are all legal.\n",
    "\n",
    "In fact, we compute something slightly more general: for each word we compute its legality.\n",
    "\n",
    "The legality of a word is the percentage of legal grams with respect to the total number of grams in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bef26a82-bbfc-4f36-902b-a3e2e3719844",
   "metadata": {},
   "outputs": [],
   "source": [
    "LEGAL_FORM = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d3b9494d-bf48-48b8-9793-dd672d2f090e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLegality():\n",
    "    LEGAL_FORM.clear()\n",
    "    \n",
    "    for (form, info) in GRAM_INDEX.items():\n",
    "        legal = 0\n",
    "        for (n, grams) in info.items():\n",
    "            legal += int(round(100 * sum(1 for gram in grams if gram in LEGAL_GRAM[n]) / len(grams)))\n",
    "        legal = int(round(legal / 2))\n",
    "        LEGAL_FORM[form] = legal\n",
    "        \n",
    "    print(f\"{len(GRAM_INDEX)} word forms with legality distributed as:\")\n",
    "    with open(f\"{POST_DIR}/legality.tsv\", \"w\") as fh:\n",
    "        distribution = collections.Counter()\n",
    "        for (form, leg) in sorted(LEGAL_FORM.items(), key=lambda x: (-x[1], x[0])):\n",
    "            fh.write(f\"{form}\\t{leg}\\n\")\n",
    "            distribution[leg] += 1\n",
    "        showDistribution(distribution, f\"word form\", \"legality\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5b63942d-e165-457c-8dd0-303c2a9ca9ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23995 word forms with legality distributed as:\n",
      "  16442 word forms with legality >=     100\n",
      "   3267 word forms with legality >=      90\n",
      "   1493 word forms with legality >=      80\n",
      "    714 word forms with legality >=      70\n",
      "    396 word forms with legality >=      60\n",
      "    238 word forms with legality >=      50\n",
      "    120 word forms with legality >=      40\n",
      "     39 word forms with legality >=      30\n",
      "     68 word forms with legality >=      20\n",
      "    111 word forms with legality >=      10\n",
      "      2 word forms with legality >=       8\n",
      "      3 word forms with legality >=       7\n",
      "      2 word forms with legality >=       6\n",
      "   1100 word forms with legality >=       0\n"
     ]
    }
   ],
   "source": [
    "getLegality()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
